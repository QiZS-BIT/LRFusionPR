# LRFusionPR
This repository is the official implementation of our paper accepted by IEEE RAL [[IEEE Xplore]](https://ieeexplore.ieee.org/document/11177016) [[arxiv]](https://arxiv.org/abs/2504.19186).

## Introduction
**LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition**

[Zhangshuo Qi](https://github.com/QiZS-BIT), [Luqi Cheng](https://github.com/ChengLuqi), [Zijie Zhou](https://github.com/ZhouZijie77), [Guangming Xiong*](https://ieeexplore.ieee.org/author/37286205000)

![image](https://github.com/QiZS-BIT/LRFusionPR/blob/main/assets/motivation.png)

LRFusionPR improves recognition accuracy and robustness by fusing LiDAR with either single-chip or scanning radar.
Extensive evaluations on multiple datasets demonstrate that our LRFusionPR achieves accurate place recognition, while 
maintaining robustness under varying weather conditions.

## Table of Contents
- [LRFusionPR](#LRFusionPR)
    - [Introduction](#introduction)
    - [Installation](#installation)
    - [Data Preparation](#data-preparation)
    - [Training](#training)
    - [Evaluation](#evaluation)
    - [Download](#download)
    - [Citation](#citation)
    - [Acknowledgement](#acknowledgement)

## Installation
* Ubuntu 20.04 + Python 3.8
* CUDA 11.3 + Pytorch 1.12.0
```
git clone https://github.com/QiZS-BIT/LRFusionPR.git
cd LRFusionPR
conda create -n lrfusionpr python=3.8
conda activate lrfusionpr
pip install -r requirements.txt
```

## Data Preparation
### nuScenes
* Download the official [nuScenes dataset](https://www.nuscenes.org/nuscenes).
* Download the [training indexes of nuScenes-BostonSeaport](https://drive.google.com/drive/folders/15PHPzwaj3_qNSJHcrVV6brslWH7dOTLJ?usp=sharing) generated by AutoPlace.
* Generate the infos, indexes and polar BEVs needed to run the code.
```
cd dataset/NuScenes
python gen_info.py
python gen_index.py
python gen_bev.py
```
* The final data structure should be like:
```
data
├── bs_bev
├── bs_db.npy
├── bs_test_query.npy
├── bs_train_query.npy
├── bs_val_query.npy
├── nuscenes_infos-bs.pkl
├── nuscenes_infos-son.pkl
├── nuscenes_infos-sq.pkl
├── son_bev
├── son_db.npy
├── son_test_query.npy
├── son_whole.npy
├── sq_bev
├── sq_db.npy
├── sq_test_query.npy
└── sq_whole.npy
```
* To generate data for simulated foggy conditions, you need to run the 
following script provided by SeeingThroughFog:
```
cd dataset/NuScenes
python fog_model/lidar_foggification.py
python gen_bev_fog.py
```

### MulRan
* Download the official [MulRan dataset](https://sites.google.com/view/mulran-pr/dataset).
* Generate the infos, indexes and polar BEVs needed to run the code.
```
cd dataset/MulRan
python gen_info.py
python gen_index.py
python gen_bev.py
```
* The final data structure should be like:
```
data
├── dcc_multi_bev
├── dcc_test_db.npy
├── dcc_test_query.npy
├── dcc_whole.npy
├── mulran_infos-dcc.pkl
├── mulran_infos-riverside.pkl
├── mulran_infos-sejong.pkl
├── riverside_multi_bev
├── riverside_test_db.npy
├── riverside_test_query.npy
├── riverside_whole.npy
├── sejong_db.npy
├── sejong_multi_bev
├── sejong_test_db.npy
├── sejong_test_query.npy
├── sejong_train_query.npy
└── sejong_whole.npy
```
* To generate data for simulated foggy conditions, you need to run the 
following script provided by SeeingThroughFog:
```
cd dataset/MulRan
python fog_model/lidar_foggification.py
python gen_bev_fog.py
```

### Oxford Radar Robotcar
* Download the official [Oxford Radar RobotCar dataset](https://oxford-robotics-institute.github.io/radar-robotcar-dataset/downloads).
* Generate the infos, indexes and polar BEVs needed to run the code.
```
cd dataset/OxfordRadar
python gen_info.py
python gen_index.py
python gen_bev.py
```
* The final data structure should be like:
```
data
├── oxford_infos-2019-01-11-13-24-51.pkl
├── oxford_infos-2019-01-11-13-24-51_whole.npy
├── oxford_infos-2019-01-11-13-24-51_db.npy
├── oxford_infos-2019-01-11-13-24-51_test_query.npy
└── oxford_infos-01-11-13-24-51_bev
```

## Training
To train the model from scratch, you first need to modify ``config/params_nusc.py``. There are three main changes to pay attention to:

* Ensure that the file paths used for training are kept, while other file paths are commented out.

* Set ``self.checkpoint_path = ""`` and configure ``self.resume_checkpoint = False``.

* Set ``self.training_root`` to the desired path for storing training files.

Then, run the following script to train the model:
```
python train_nusc.py
```

For training on the MulRan dataset, similar modifications need to be applied to the ``config/params_mulran.py`` file.

Then, run the following script to train the model:
```
python train_mulran.py
```

## Evaluation
You can either evaluate our provided pre-trained weights, or evaluate results obtained from training locally.

First, ensure that in ``config/params_nusc.py``/``config/params_mulran.py``/``config/params_oxford.py``, the file paths 
used for evaluating specific sequences are kept, while other file paths are commented out.

Then proceed with one of the following options:

**Option 1: Evaluate Pre-trained Weights**

* Set ``self.checkpoint_path`` to the path of our provided pre-trained weights.

**Option 2: Evaluate Locally Trained Results**

* Set self.checkpoint_path = ""

* Set ``self.training_root`` to the folder where training results are stored.

* Set ``self.resume_epoch`` to the epoch corresponding to the weights you want to evaluate.

After setting ``self.resume_checkpoint = True``, run the following scripts depending on which dataset you want to evaluate:
```
python test_nusc.py
python test_mulran.py
python test_oxford.py
```

## Download
* You can download our generated infos, indexes and BEVs from this [link](https://drive.google.com/file/d/1kJVL1ZRyja3QH-9-8onnFwRsOYdMZoxT/view?usp=sharing).
* Our pre-trained weights are available at this [link](https://drive.google.com/file/d/1-A48nmG6mmu80mA2g6B4N36L0fOJ8aW3/view?usp=sharing).

## TODO
- [X] Release the [paper](https://arxiv.org/abs/2504.19186)
- [X] Release the data preparation code
- [X] Release the training and evaluation code, and our pretrained model

## Citation
If you find this project useful for your research, please consider citing:
```
@article{qi2025lrfusionpr,
  title={LRFusionPR: A Polar BEV-Based LiDAR-Radar Fusion Network for Place Recognition},
  author={Qi, Zhangshuo and Cheng, Luqi and Zhou, Zijie and Xiong, Guangming},
  journal={arXiv preprint arXiv:2504.19186},
  year={2025}
}
```

## Acknowledgement
Many thanks to these excellent projects:
* [BEVPlace](https://github.com/zjuluolun/BEVPlace2)
* [LCPR](https://github.com/ZhouZijie77/LCPR)
* [AutoPlace](https://github.com/ramdrop/autoplace)
* [EgoNN](https://github.com/jac99/Egonn)
* [SeeingThroughFog](https://github.com/princeton-computational-imaging/SeeingThroughFog)
